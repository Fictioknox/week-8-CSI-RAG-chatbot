# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13zHG3VtewsSuW3zPosq9WL51uZ3g8WwY
"""

!pip install -U langchain-community langchain faiss-cpu sentence-transformers transformers pandas

import pandas as pd

df = pd.read_csv("Training Dataset.csv")

# Convert rows to readable strings
texts = []
for _, row in df.iterrows():
    row_text = ", ".join([f"{col}: {row[col]}" for col in df.columns])
    texts.append(row_text)

print("Sample row text:\n", texts[0])

# Load poem text
with open("IF.txt", "r", encoding="utf-8") as f:
    poem_text = f.read()

# Optionally chunk long poem
from langchain.text_splitter import CharacterTextSplitter

poem_chunks = CharacterTextSplitter(chunk_size=500, chunk_overlap=0).split_text(poem_text)

# Add to main text list
texts.extend(poem_chunks)

from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Chunk data
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)
documents = splitter.create_documents(texts)

# Embedding model (lightweight & fast)
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Create vector store
vectorstore = FAISS.from_documents(documents, embedding)

from transformers import pipeline

# FLAN-T5 is a free and friendly model for Q&A
llm_pipeline = pipeline("text2text-generation", model="google/flan-t5-base", max_length=256)

def answer_question(query):
    # Step 1: Retrieve relevant chunks
    docs = vectorstore.similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    # Step 2: Generate answer using LLM
    prompt = f"Answer the question based on the following data:\n{context}\n\nQuestion: {query}"
    result = llm_pipeline(prompt)[0]['generated_text']
    return result

while True:
    query = input("Ask a question (or 'exit'): ")
    if query.lower() == "exit":
        break
    response = answer_question(query)
    print("Answer:", response)

